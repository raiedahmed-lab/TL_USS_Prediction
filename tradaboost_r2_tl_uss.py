# -*- coding: utf-8 -*-
"""TrAdaboost.R2 - TL_USS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YrO0IQ1qAjxe7KflfL69oxyXVOdYi77U

# Import Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV

"""# Load Data"""

source_df = pd.read_csv('/content/Source.csv')
target_df = pd.read_csv('/content/Target.csv')

"""# Evaluation Function"""

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    nonzero_idx = y_true != 0
    if not np.any(nonzero_idx):
        return np.inf
    return np.mean(np.abs((y_true[nonzero_idx] - y_pred[nonzero_idx]) / y_true[nonzero_idx])) * 100

def print_regression_metrics(y_true, y_pred, dataset_name="Dataset"):
    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)

    print(f"=== {dataset_name} Evaluation ===")
    print(f"R² Score      : {r2:.4f}")
    print(f"MAE           : {mae:.4f}")
    print(f"MSE           : {mse:.4f}")
    print(f"RMSE          : {rmse:.4f}")
    print(f"MAPE (%)      : {mape:.4f}")
    print("-------------------------------")

"""# Organize Data into the Required Dictionaries"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# -------------------------
# Step 1: Split SUST data
# -------------------------
sust_features = target_df.drop(columns=['USS'])
sust_labels = target_df['USS']

X_sust_train_raw, X_sust_test_raw, y_sust_train, y_sust_test = train_test_split(
    sust_features, sust_labels, test_size=0.2, random_state=42
)

# -------------------------
# Step 2: Scaling (fit on source, apply to both source & target)
# -------------------------
scaler = StandardScaler()

X_source = source_df.drop(columns=['USS']).values
y_source = source_df['USS'].values

# Fit scaler on source domain
X_source = scaler.fit_transform(X_source)

# Apply same transformation to target train and test
X_sust_train = scaler.transform(X_sust_train_raw)
X_sust_test = scaler.transform(X_sust_test_raw)

# -------------------------
# Step 3: Wrap for Bin Cao's code format
# -------------------------
Multi_trans_A = {
    'trans_A_1': X_source
}
Multi_response_A = {
    'response_A_1': y_source
}

trans_S = X_sust_train
response_S = y_sust_train.values  # Ensure this is a numpy array
X_test = X_sust_test
y_test = y_sust_test.values       # Ensure this is a numpy array

"""# TrAdaboost.R2 (from Github: Bin-Cao/TrAdaboost)

All defined functions remain the same.

*Only modificaiton inside the baselearner (DT) parameters*
"""

N = 30

import numpy as np
from sklearn.tree import DecisionTreeRegressor

def TrAdaBoost_R2(trans_S, Multi_trans_A, response_S, Multi_response_A, test, N,
                  max_depth=3, min_samples_leaf=5, random_state=0, return_models=False):
    """TrAdaBoost.R2 implementation using DecisionTreeRegressor as base regressor."""

    # Prepare source features
    trans_A = list(Multi_trans_A.values())[0]
    for key in list(Multi_trans_A.keys())[1:]:
        trans_A = np.concatenate((trans_A, Multi_trans_A[key]), axis=0)

    # Prepare source labels
    response_A = list(Multi_response_A.values())[0]
    for key in list(Multi_response_A.keys())[1:]:
        response_A = np.concatenate((response_A, Multi_response_A[key]), axis=0)

    # Combine source and target training data
    trans_data = np.concatenate((trans_A, trans_S), axis=0)
    trans_response = np.concatenate((response_A, response_S), axis=0)

    row_A = trans_A.shape[0]
    row_S = trans_S.shape[0]
    row_T = test.shape[0]

    if N > row_A:
        raise ValueError(f"The number of estimators (N={N}) should be <= number of source samples ({row_A})")

    test_data = np.concatenate((trans_data, test), axis=0)

    # Initialize weights
    weights_A = np.ones((row_A, 1)) / row_A
    weights_S = np.ones((row_S, 1)) / row_S
    weights = np.concatenate((weights_A, weights_S), axis=0)

    # Initialization of hyperparameters
    beta = 1 / (1 + np.sqrt(2 * np.log(row_A / N)))
    beta_T = np.zeros((1, N))
    result_response = np.ones((row_A + row_S + row_T, N))
    predict = np.zeros(row_T)

    print("TrAdaBoost.R2 started with DecisionTreeRegressor base estimator.")
    print('=' * 60)

    trans_data = np.asarray(trans_data, order='C')
    trans_response = np.asarray(trans_response, order='C')
    test_data = np.asarray(test_data, order='C')

    for i in range(N):
        weights = calculate_P(weights)

        # Use consistent DecisionTreeRegressor config
        reg = DecisionTreeRegressor(
            max_depth=max_depth,
            min_samples_leaf=5,
            splitter='best',
            max_features=None,
            random_state=random_state
        )
        reg.fit(trans_data, trans_response, sample_weight=weights[:, 0])
        result_response[:, i] = reg.predict(test_data)

        error_rate = calculate_error_rate(
            response_S,
            result_response[row_A:row_A + row_S, i],
            weights[row_A:row_A + row_S, 0]
        )

        if error_rate <= 1e-10 or error_rate > 0.5:
            N = i
            break

        beta_T[0, i] = error_rate / (1 - error_rate)

        print(f"Iter {i+1}/{N} - Error: {error_rate:.4f}, Beta_T: {beta_T[0, i]:.4f}")

        D_t = np.abs(result_response[:row_A + row_S, i] - trans_response).max()

        # Update weights (target)
        for j in range(row_S):
            diff = np.abs(result_response[row_A + j, i] - response_S[j]) / D_t
            weights[row_A + j] *= np.power(beta_T[0, i], -diff)

        # Update weights (source)
        for j in range(row_A):
            diff = np.abs(result_response[j, i] - response_A[j]) / D_t
            weights[j] *= np.power(beta, diff)

    for i in range(row_T):
        predict[i] = np.mean(result_response[row_A + row_S + i, int(np.floor(N / 2)):N])

    print("TrAdaBoost_R2 is done")
    print('=' * 60)
    return predict


def calculate_P(weights):
    total = np.sum(weights)
    return weights / total


def calculate_error_rate(response_R, response_H, weight):
    total = np.abs(response_R - response_H).max()
    return np.sum(weight * np.abs(response_R - response_H) / total)

"""#Results"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import itertools

# Parameter grid
max_depths = [2, 3, 4, 5]
n_estimators_list = [10, 25, 50, 75, 100]

# Store best results
best_tradaboost = {'r2': -np.inf}
best_train_pred = None
best_test_pred = None

# Grid search over hyperparameters
for max_depth, n_est in itertools.product(max_depths, n_estimators_list):
    print(f"Training TrAdaBoost.R2: max_depth={max_depth}, n_estimators={n_est}")

    # Predict on training set
    y_train_pred = TrAdaBoost_R2(
        trans_S=trans_S,
        Multi_trans_A=Multi_trans_A,
        response_S=response_S,
        Multi_response_A=Multi_response_A,
        test=trans_S,
        N=n_est,
        max_depth=max_depth,             # Explicitly pass max_depth
        random_state=42                  # For reproducibility
    )

    # Predict on test set
    y_test_pred = TrAdaBoost_R2(
        trans_S=trans_S,
        Multi_trans_A=Multi_trans_A,
        response_S=response_S,
        Multi_response_A=Multi_response_A,
        test=X_test,
        N=n_est,
        max_depth=max_depth,             # Same as above
        random_state=42
    )

    # Evaluation on test set
    r2 = r2_score(y_test, y_test_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    mae = mean_absolute_error(y_test, y_test_pred)
    mse = mean_squared_error(y_test, y_test_pred)

    # Track best performing configuration
    if r2 > best_tradaboost['r2']:
        best_tradaboost = {
            'r2': r2,
            'rmse': rmse,
            'mae': mae,
            'mse': mse,
            'params': {'max_depth': max_depth, 'n_estimators': n_est}
        }
        best_train_pred = y_train_pred
        best_test_pred = y_test_pred

# ==================== Print Best Results ==========================
print("\n=== Best TrAdaBoost.R2 Results ===")
print("Params       :", best_tradaboost['params'])
print("R² Score     :", round(best_tradaboost['r2'], 4))
print("RMSE         :", round(best_tradaboost['rmse'], 4))
print("MSE          :", round(best_tradaboost['mse'], 4))
print("MAE          :", round(best_tradaboost['mae'], 4))

# =================== Print Evaluation Metrics =====================
print_regression_metrics(response_S, best_train_pred, dataset_name="TrAdaBoost.R2 Train Set")
print_regression_metrics(y_test, best_test_pred, dataset_name="TrAdaBoost.R2 Test Set")

"""# ABLATION STUDY"""

# Source (different distribution domain: Finland)
X_source = source_df.drop(columns=['USS']).values
y_source = source_df['USS'].values

# Target (same distribution domain: SUST)
X_target = target_df.drop(columns=['USS']).values
y_target = target_df['USS'].values

from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the source features
X_source = scaler.fit_transform(X_source)

# Transform the target features using the scaler fitted on source features
X_target = scaler.transform(X_target)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import r2_score, mean_squared_error

# ================== Font Config ==================
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']

# ================== Split Target Data ==================
X_target = target_df[['LL', 'PL', 'PI', 'Moisture', 'Density']].values
y_target = target_df['USS'].values
Xa_full, Xt, ya_full, yt = train_test_split(X_target, y_target, test_size=0.2, random_state=42)

# ================== Scale Data ==================
scaler = StandardScaler()
X_source_scaled = scaler.fit_transform(X_source)  # Fit on source
Xa_full_scaled = scaler.transform(Xa_full)
Xt_scaled = scaler.transform(Xt)

# ================== Best Params (from previous tuning) ==================
best_params_tradaboost = {'max_depth': 2, 'n_estimators': 50}

# ================== Baseline AdaBoost ==================
adaboost_params = {
    'estimator': DecisionTreeRegressor(max_depth=3, min_samples_leaf=2),
    'n_estimators': 50,
    'random_state': 42
}

# ================== Ablation Setup ==================
target_percents = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
n_runs = 40
min_target_samples = 5  # avoid unstable training

# Arrays to store scores
r2_scores_tradaboost_all = np.zeros((n_runs, len(target_percents)))
r2_scores_adaboost_all = np.zeros((n_runs, len(target_percents)))
rmse_scores_tradaboost_all = np.zeros((n_runs, len(target_percents)))
rmse_scores_adaboost_all = np.zeros((n_runs, len(target_percents)))

# ================== TrAdaBoost Functions (your previous working version) ==================
def calculate_P(weights):
    return weights / np.sum(weights)

def calculate_error_rate(response_R, response_H, weight):
    total = np.abs(response_R - response_H).max()
    return np.sum(weight * np.abs(response_R - response_H) / total)

def TrAdaBoost_R2(trans_S, Multi_trans_A, response_S, Multi_response_A, test, N,
                  max_depth=3, min_samples_leaf=5, random_state=0, return_models=False):
    trans_A = list(Multi_trans_A.values())[0]
    response_A = list(Multi_response_A.values())[0]

    trans_data = np.concatenate((trans_A, trans_S), axis=0)
    trans_response = np.concatenate((response_A, response_S), axis=0)

    row_A = trans_A.shape[0]
    row_S = trans_S.shape[0]
    row_T = test.shape[0]

    beta = 1 / (1 + np.sqrt(2 * np.log(row_A / N)))
    beta_T = np.zeros((1, N))
    result_response = np.ones((row_A + row_S + row_T, N))
    predict = np.zeros(row_T)

    weights_A = np.ones((row_A, 1)) / row_A
    weights_S = np.ones((row_S, 1)) / row_S
    weights = np.concatenate((weights_A, weights_S), axis=0)

    test_data = np.concatenate((trans_data, test), axis=0)

    for i in range(N):
        weights = calculate_P(weights)

        reg = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=random_state)
        reg.fit(trans_data, trans_response, sample_weight=weights[:,0])
        result_response[:, i] = reg.predict(test_data)

        error_rate = calculate_error_rate(response_S, result_response[row_A:row_A+row_S, i], weights[row_A:row_A+row_S,0])
        if error_rate <= 1e-10 or error_rate > 0.5:
            N = i
            break

        beta_T[0, i] = error_rate / (1 - error_rate)
        D_t = np.abs(result_response[:row_A + row_S, i] - trans_response).max()

        # Update weights
        for j in range(row_S):
            diff = np.abs(result_response[row_A + j, i] - response_S[j]) / D_t
            weights[row_A + j] *= np.power(beta_T[0, i], -diff)
        for j in range(row_A):
            diff = np.abs(result_response[j, i] - response_A[j]) / D_t
            weights[j] *= np.power(beta, diff)

    for i in range(row_T):
        predict[i] = np.mean(result_response[row_A + row_S + i, int(np.floor(N/2)):N])

    return predict

# ================== Main Ablation Loop ==================
for i in range(n_runs):
    shuffled_indices = np.random.permutation(len(Xa_full_scaled))
    Xa_full_shuffled = Xa_full_scaled[shuffled_indices]
    ya_full_shuffled = ya_full[shuffled_indices]

    for j, perc in enumerate(target_percents):
        n_samples = max(int(len(Xa_full_shuffled) * perc), min_target_samples)
        Xa_sub = Xa_full_shuffled[:n_samples]
        ya_sub = ya_full_shuffled[:n_samples]

        # TrAdaBoost: N based on source samples
        N_tradaboost = min(best_params_tradaboost['n_estimators'], len(X_source_scaled))

        # Wrap source for TrAdaBoost
        multi_trans_a_sub = {'trans_A_1': X_source_scaled}
        multi_response_a_sub = {'response_A_1': y_source}

        # Predict
        y_pred_tradaboost = TrAdaBoost_R2(
            trans_S=Xa_sub,
            Multi_trans_A=multi_trans_a_sub,
            response_S=ya_sub,
            Multi_response_A=multi_response_a_sub,
            test=Xt_scaled,
            N=N_tradaboost,
            max_depth=best_params_tradaboost['max_depth'],
            random_state=42
        )

        # Baseline AdaBoost
        model_adaboost = AdaBoostRegressor(**adaboost_params)
        model_adaboost.fit(Xa_sub, ya_sub)
        y_pred_adaboost = model_adaboost.predict(Xt_scaled)

        # Store metrics
        r2_scores_tradaboost_all[i, j] = r2_score(yt, y_pred_tradaboost)
        rmse_scores_tradaboost_all[i, j] = np.sqrt(mean_squared_error(yt, y_pred_tradaboost))
        r2_scores_adaboost_all[i, j] = r2_score(yt, y_pred_adaboost)
        rmse_scores_adaboost_all[i, j] = np.sqrt(mean_squared_error(yt, y_pred_adaboost))

# ================== Aggregate Metrics ==================
def aggregate_metrics(scores):
    mean_score = np.nanmean(scores, axis=0)
    low = np.nanpercentile(scores, 2.5, axis=0)
    high = np.nanpercentile(scores, 97.5, axis=0)
    return mean_score, low, high

mean_r2_tradaboost, ci_r2_tradaboost_low, ci_r2_tradaboost_high = aggregate_metrics(r2_scores_tradaboost_all)
mean_r2_adaboost, ci_r2_adaboost_low, ci_r2_adaboost_high = aggregate_metrics(r2_scores_adaboost_all)
mean_rmse_tradaboost, ci_rmse_tradaboost_low, ci_rmse_tradaboost_high = aggregate_metrics(rmse_scores_tradaboost_all)
mean_rmse_adaboost, ci_rmse_adaboost_low, ci_rmse_adaboost_high = aggregate_metrics(rmse_scores_adaboost_all)

# ================== Visualization ==================
fig, axes = plt.subplots(1,2,figsize=(10,4))
target_percents_plot = np.array(target_percents) * 100

# R²
axes[0].plot(target_percents_plot, mean_r2_tradaboost, 'o-', label="TrAdaBoost.R2", color='tab:red', linewidth=1)
axes[0].fill_between(target_percents_plot, ci_r2_tradaboost_low, ci_r2_tradaboost_high, color='tab:red', alpha=0.2)
axes[0].plot(target_percents_plot, mean_r2_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
axes[0].fill_between(target_percents_plot, ci_r2_adaboost_low, ci_r2_adaboost_high, color='tab:blue', alpha=0.18)
axes[0].set_xlabel("Target domain data (%)", fontsize=14)
axes[0].set_ylabel("R² Score", fontsize=14)
axes[0].set_title("(a) Coefficient of Determination ($R^2$)", fontsize=14)
axes[0].grid(True, linestyle='--', alpha=0.7)
axes[0].set_xticks(target_percents_plot)
axes[0].set_ylim(-1,1)
axes[0].legend()

# RMSE
axes[1].plot(target_percents_plot, mean_rmse_tradaboost, 'o-', label="TrAdaBoost.R2", color='tab:red', linewidth=1)
axes[1].fill_between(target_percents_plot, ci_rmse_tradaboost_low, ci_rmse_tradaboost_high, color='tab:red', alpha=0.2)
axes[1].plot(target_percents_plot, mean_rmse_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
axes[1].fill_between(target_percents_plot, ci_rmse_adaboost_low, ci_rmse_adaboost_high, color='tab:blue', alpha=0.18)
axes[1].set_xlabel("Target domain data (%)", fontsize=14)
axes[1].set_ylabel("RMSE", fontsize=14)
axes[1].set_title("(b) Root Mean Square Error (RMSE)", fontsize=14)
axes[1].grid(True, linestyle='--', alpha=0.7)
axes[1].set_xticks(target_percents_plot)
axes[1].set_ylim(bottom=0)
axes[1].legend()

plt.suptitle("TrAdaBoost.R2 vs Baseline AdaBoost.R2", fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(9, 3.5))
target_percents_plot = np.array(target_percents) * 100

# -------------------- R² Plot --------------------
line_tradaboost, = axes[0].plot(target_percents_plot, mean_r2_tradaboost, 'o-', label="TrAdaBoost.R2", color='tab:red', linewidth=1)
ci_tradaboost = axes[0].fill_between(target_percents_plot, ci_r2_tradaboost_low, ci_r2_tradaboost_high, color='tab:red', alpha=0.2)

line_adaboost, = axes[0].plot(target_percents_plot, mean_r2_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
ci_adaboost = axes[0].fill_between(target_percents_plot, ci_r2_adaboost_low, ci_r2_adaboost_high, color='tab:blue', alpha=0.18)

axes[0].set_xlabel("Target domain data (%)", fontsize=14)
axes[0].set_ylabel("R² Score", fontsize=14)
#axes[0].set_title("(a) Coefficient of Determination ($R^2$)", fontsize=14)
axes[0].grid(True, linestyle='--', alpha=0.7)
axes[0].set_xticks(target_percents_plot)
axes[0].set_ylim(0, 1)


axes[0].legend(
    [line_tradaboost, ci_tradaboost, line_adaboost, ci_adaboost],
    ["TrAdaBoost.R2", "TrAdaBoost.R2 CI", "Baseline AdaBoost.R2", "Baseline CI"],
    loc='lower right', fontsize=10
)

# -------------------- RMSE Plot --------------------
line_tradaboost_rmse, = axes[1].plot(target_percents_plot, mean_rmse_tradaboost, 'o-', label="TrAdaBoost.R2", color='tab:red', linewidth=1)
ci_tradaboost_rmse = axes[1].fill_between(target_percents_plot, ci_rmse_tradaboost_low, ci_rmse_tradaboost_high, color='tab:red', alpha=0.2)

line_adaboost_rmse, = axes[1].plot(target_percents_plot, mean_rmse_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
ci_adaboost_rmse = axes[1].fill_between(target_percents_plot, ci_rmse_adaboost_low, ci_rmse_adaboost_high, color='tab:blue', alpha=0.18)

axes[1].set_xlabel("Target domain data (%)", fontsize=14)
axes[1].set_ylabel("RMSE", fontsize=14)
#axes[1].set_title("(b) Root Mean Square Error (RMSE)", fontsize=14)
axes[1].grid(True, linestyle='--', alpha=0.7)
axes[1].set_xticks(target_percents_plot)
axes[1].set_ylim(bottom=0)

# Create legend including CI
axes[1].legend(
    [line_tradaboost_rmse, ci_tradaboost_rmse, line_adaboost_rmse, ci_adaboost_rmse],
    ["TrAdaBoost.R2", "TrAdaBoost.R2 CI", "Baseline AdaBoost.R2", "Baseline CI"],
    loc='lower right', fontsize=10
)

#plt.suptitle("TrAdaBoost.R2 vs Baseline AdaBoost.R2", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

fig.savefig('tradaboost vs baseline performance v2.svg', format='svg', dpi=1200)