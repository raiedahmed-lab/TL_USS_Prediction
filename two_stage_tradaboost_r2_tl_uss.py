# -*- coding: utf-8 -*-
"""Two Stage TrAdaboost.R2 - TL_USS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qi3SA_HKrE4-ff5JLAQj5Z8jVbvgOHnM
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

source_df = pd.read_csv('/content/Source.csv')
target_df = pd.read_csv('/content/Target.csv')

"""#Define Two Stage TrAdaboost.R2

Code Source: [Jie Ren Github](https://github.com/jay15summer/Two-stage-TrAdaboost.R2/blob/master/README.md)


> Author: Jie Ren

Department of Industrial and Manufacturing Engineering,

Florida State University

[Google Scholar](https://scholar.google.com/citations?user=9pqclwIAAAAJ&hl=en&oi=ao)
"""

"""
TwoStageTrAdaBoostR2 algorithm

based on algorithm 3 in paper "Boosting for Regression Transfer".

"""

import numpy as np
import copy
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

################################################################################
## the second stage
################################################################################
class Stage2_TrAdaBoostR2:
    def __init__(self,
                 base_estimator = DecisionTreeRegressor(max_depth=4),
                 sample_size = None,
                 n_estimators = 50,
                 learning_rate = 1.,
                 loss = 'linear',
                 random_state = np.random.mtrand._rand):
        self.base_estimator = base_estimator
        self.sample_size = sample_size
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.loss = loss
        self.random_state = random_state


    def fit(self, X, y, sample_weight=None):
        # Check parameters
        if self.learning_rate <= 0:
            raise ValueError("learning_rate must be greater than zero")

        if sample_weight is None:
            # Initialize weights to 1 / n_samples
            sample_weight = np.empty(X.shape[0], dtype=np.float64)
            sample_weight[:] = 1. / X.shape[0]
        else:
            # Normalize existing weights
            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)

            # Check that the sample weights sum is positive
            if sample_weight.sum() <= 0:
                raise ValueError(
                      "Attempting to fit with a non-positive "
                      "weighted number of samples.")

        if self.sample_size is None:
            raise ValueError("Additional input required: sample size of source and target is missing")
        elif np.array(self.sample_size).sum() != X.shape[0]:
            raise ValueError("Input error: the specified sample size does not equal to the input size")

        # Clear any previous fit results
        self.estimators_ = []
        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)
        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)

        for iboost in range(self.n_estimators): # this for loop is sequential and does not support parallel(revison is needed if making parallel)
            # Boosting step
            sample_weight, estimator_weight, estimator_error = self._stage2_adaboostR2(
                    iboost,
                    X, y,
                    sample_weight)
            # Early termination
            if sample_weight is None:
                break

            self.estimator_weights_[iboost] = estimator_weight
            self.estimator_errors_[iboost] = estimator_error

            # Stop if error is zero
            if estimator_error == 0:
                break

            sample_weight_sum = np.sum(sample_weight)

            # Stop if the sum of sample weights has become non-positive
            if sample_weight_sum <= 0:
                break

            if iboost < self.n_estimators - 1:
                # Normalize
                sample_weight /= sample_weight_sum
        return self


    def _stage2_adaboostR2(self, iboost, X, y, sample_weight):

        estimator = copy.deepcopy(self.base_estimator) # some estimators allow for specifying random_state estimator = base_estimator(random_state=random_state)

        ## using sampling method to account for sample_weight as discussed in Drucker's paper
        # Weighted sampling of the training set with replacement
        cdf = np.cumsum(sample_weight)
        cdf /= cdf[-1]
        uniform_samples = self.random_state.random_sample(X.shape[0])
        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
        # searchsorted returns a scalar
        bootstrap_idx = np.array(bootstrap_idx, copy=False)

        # Fit on the bootstrapped sample and obtain a prediction
        # for all samples in the training set
        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
        y_predict = estimator.predict(X)

        self.estimators_.append(estimator)  # add the fitted estimator

        error_vect = np.abs(y_predict - y)
        error_max = error_vect.max()

        if error_max != 0.:
            error_vect /= error_max

        if self.loss == 'square':
            error_vect **= 2
        elif self.loss == 'exponential':
            error_vect = 1. - np.exp(- error_vect)

        # Calculate the average loss
        estimator_error = (sample_weight * error_vect).sum()

        if estimator_error <= 0:
            # Stop if fit is perfect
            return sample_weight, 1., 0.

        elif estimator_error >= 0.5:
            # Discard current estimator only if it isn't the only one
            if len(self.estimators_) > 1:
                self.estimators_.pop(-1)
            return None, None, None

        beta = estimator_error / (1. - estimator_error)

        # avoid overflow of np.log(1. / beta)
        if beta < 1e-308:
            beta = 1e-308
        estimator_weight = self.learning_rate * np.log(1. / beta)

        # Boost weight using AdaBoost.R2 alg except the weight of the source data
        # the weight of the source data are remained
        source_weight_sum= np.sum(sample_weight[:-self.sample_size[-1]]) / np.sum(sample_weight)
        target_weight_sum = np.sum(sample_weight[-self.sample_size[-1]:]) / np.sum(sample_weight)

        if not iboost == self.n_estimators - 1:
            sample_weight[-self.sample_size[-1]:] *= np.power(
                    beta,
                    (1. - error_vect[-self.sample_size[-1]:]) * self.learning_rate)
            # make the sum weight of the source data not changing
            source_weight_sum_new = np.sum(sample_weight[:-self.sample_size[-1]]) / np.sum(sample_weight)
            target_weight_sum_new = np.sum(sample_weight[-self.sample_size[-1]:]) / np.sum(sample_weight)
            if source_weight_sum_new != 0. and target_weight_sum_new != 0.:
                sample_weight[:-self.sample_size[-1]] = sample_weight[:-self.sample_size[-1]]*source_weight_sum/source_weight_sum_new
                sample_weight[-self.sample_size[-1]:] = sample_weight[-self.sample_size[-1]:]*target_weight_sum/target_weight_sum_new

        return sample_weight, estimator_weight, estimator_error


    def predict(self, X):
        # Evaluate predictions of all estimators
        predictions = np.array([
                est.predict(X) for est in self.estimators_[:len(self.estimators_)]]).T

        # Sort the predictions
        sorted_idx = np.argsort(predictions, axis=1)

        # Find index of median prediction for each sample
        weight_cdf = np.cumsum(self.estimator_weights_[sorted_idx], axis=1)
        median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
        median_idx = median_or_above.argmax(axis=1)

        median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]

        # Return median predictions
        return predictions[np.arange(X.shape[0]), median_estimators]


################################################################################
## the whole two stages
################################################################################
class TwoStageTrAdaBoostR2:
    def __init__(self,
                 base_estimator = DecisionTreeRegressor(max_depth=4),
                 sample_size = None,
                 n_estimators = 50,
                 steps = 10,
                 fold = 5,
                 learning_rate = 1.,
                 loss = 'linear',
                 random_state = np.random.mtrand._rand):
        self.base_estimator = base_estimator
        self.sample_size = sample_size
        self.n_estimators = n_estimators
        self.steps = steps
        self.fold = fold
        self.learning_rate = learning_rate
        self.loss = loss
        self.random_state = random_state


    def fit(self, X, y, sample_weight=None):
        # Check parameters
        if self.learning_rate <= 0:
            raise ValueError("learning_rate must be greater than zero")

        if sample_weight is None:
            # Initialize weights to 1 / n_samples
            sample_weight = np.empty(X.shape[0], dtype=np.float64)
            sample_weight[:] = 1. / X.shape[0]
        else:
            # Normalize existing weights
            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)

            # Check that the sample weights sum is positive
            if sample_weight.sum() <= 0:
                raise ValueError(
                      "Attempting to fit with a non-positive "
                      "weighted number of samples.")

        if self.sample_size is None:
            raise ValueError("Additional input required: sample size of source and target is missing")
        elif np.array(self.sample_size).sum() != X.shape[0]:
            raise ValueError("Input error: the specified sample size does not equal to the input size")


        X_source = X[:-self.sample_size[-1]]
        y_source = y[:-self.sample_size[-1]]
        X_target = X[-self.sample_size[-1]:]
        y_target = y[-self.sample_size[-1]:]

        self.models_ = []
        self.errors_ = []
        for istep in range(self.steps):
            model = Stage2_TrAdaBoostR2(self.base_estimator,
                                        sample_size = self.sample_size,
                                        n_estimators = self.n_estimators,
                                        learning_rate = self.learning_rate, loss = self.loss,
                                        random_state = self.random_state)
            model.fit(X, y, sample_weight = sample_weight)
            self.models_.append(model)
            # cv training
            kf = KFold(n_splits = self.fold)
            error = []
            target_weight = sample_weight[-self.sample_size[-1]:]
            source_weight = sample_weight[:-self.sample_size[-1]]
            for train, test in kf.split(X_target):
                sample_size = [self.sample_size[0], len(train)]
                model = Stage2_TrAdaBoostR2(self.base_estimator,
                                        sample_size = sample_size,
                                        n_estimators = self.n_estimators,
                                        learning_rate = self.learning_rate, loss = self.loss,
                                        random_state = self.random_state)
                X_train = np.concatenate((X_source, X_target[train]))
                y_train = np.concatenate((y_source, y_target[train]))
                X_test = X_target[test]
                y_test = y_target[test]
                # make sure the sum weight of the target data do not change with CV's split sampling
                target_weight_train = target_weight[train]*np.sum(target_weight)/np.sum(target_weight[train])
                model.fit(X_train, y_train, sample_weight = np.concatenate((source_weight, target_weight_train)))
                y_predict = model.predict(X_test)
                error.append(mean_squared_error(y_predict, y_test))

            self.errors_.append(np.array(error).mean())

            sample_weight = self._twostage_adaboostR2(istep, X, y, sample_weight)

            if sample_weight is None:
                break
            if np.array(error).mean() == 0:
                break

            sample_weight_sum = np.sum(sample_weight)

            # Stop if the sum of sample weights has become non-positive
            if sample_weight_sum <= 0:
                break

            if istep < self.steps - 1:
                # Normalize
                sample_weight /= sample_weight_sum
        return self


    def _twostage_adaboostR2(self, istep, X, y, sample_weight):

        estimator = copy.deepcopy(self.base_estimator) # some estimators allow for specifying random_state estimator = base_estimator(random_state=random_state)

        ## using sampling method to account for sample_weight as discussed in Drucker's paper
        # Weighted sampling of the training set with replacement
        cdf = np.cumsum(sample_weight)
        cdf /= cdf[-1]
        uniform_samples = self.random_state.random_sample(X.shape[0])
        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
        # searchsorted returns a scalar
        bootstrap_idx = np.array(bootstrap_idx, copy=False)

        # Fit on the bootstrapped sample and obtain a prediction
        # for all samples in the training set
        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
        y_predict = estimator.predict(X)


        error_vect = np.abs(y_predict - y)
        error_max = error_vect.max()

        if error_max != 0.:
            error_vect /= error_max

        if self.loss == 'square':
            error_vect **= 2
        elif self.loss == 'exponential':
            error_vect = 1. - np.exp(- error_vect)

        # Update the weight vector
        beta = self._beta_binary_search(istep, sample_weight, error_vect, stp = 1e-30)

        if not istep == self.steps - 1:
            sample_weight[:-self.sample_size[-1]] *= np.power(
                    beta,
                    (error_vect[:-self.sample_size[-1]]) * self.learning_rate)
        return sample_weight


    def _beta_binary_search(self, istep, sample_weight, error_vect, stp):
        # calculate the specified sum of weight for the target data
        n_target = self.sample_size[-1]
        n_source = np.array(self.sample_size).sum() - n_target
        theoretical_sum = n_target/(n_source+n_target) + istep/(self.steps-1)*(1-n_target/(n_source+n_target))
        # for the last iteration step, beta is 0.
        if istep == self.steps - 1:
            beta = 0.
            return beta
        # binary search for beta
        L = 0.
        R = 1.
        beta = (L+R)/2
        sample_weight_ = copy.deepcopy(sample_weight)
        sample_weight_[:-n_target] *= np.power(
                    beta,
                    (error_vect[:-n_target]) * self.learning_rate)
        sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)
        updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)

        while np.abs(updated_weight_sum - theoretical_sum) > 0.01:
            if updated_weight_sum < theoretical_sum:
                R = beta - stp
                if R > L:
                    beta = (L+R)/2
                    sample_weight_ = copy.deepcopy(sample_weight)
                    sample_weight_[:-n_target] *= np.power(
                                beta,
                                (error_vect[:-n_target]) * self.learning_rate)
                    sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)
                    updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)
                else:
                    print("At step:", istep+1)
                    print("Binary search's goal not meeted! Value is set to be the available best!")
                    print("Try reducing the search interval. Current stp interval:", stp)
                    break

            elif updated_weight_sum > theoretical_sum:
                L = beta + stp
                if L < R:
                    beta = (L+R)/2
                    sample_weight_ = copy.deepcopy(sample_weight)
                    sample_weight_[:-n_target] *= np.power(
                                beta,
                                (error_vect[:-n_target]) * self.learning_rate)
                    sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)
                    updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)
                else:
                    print("At step:", istep+1)
                    print("Binary search's goal not meeted! Value is set to be the available best!")
                    print("Try reducing the search interval. Current stp interval:", stp)
                    break
        return beta


    def predict(self, X):
        # select the model with the least CV error
        fmodel = self.models_[np.array(self.errors_).argmin()]
        predictions = fmodel.predict(X)
        return predictions

"""#Pre Processing"""

# Source (different distribution domain: Finland)
X_source = source_df.drop(columns=['USS']).values
y_source = source_df['USS'].values

# Target (same distribution domain: SUST)
X_target = target_df.drop(columns=['USS']).values
y_target = target_df['USS'].values

from sklearn.preprocessing import StandardScaler

# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the source features
X_source = scaler.fit_transform(X_source)

# Transform the target features using the scaler fitted on source features
X_target = scaler.transform(X_target)

from sklearn.model_selection import train_test_split

X_target_train, X_target_test, y_target_train, y_target_test = train_test_split(
    X_target, y_target, test_size=0.2, random_state=42
)

"""# Run Model

# Code with GridsearchCV
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import itertools

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import itertools

# ================================================
#     Manual Grid Search for Both Models
# ================================================

max_depths = [2, 3, 4]
n_estimators_list = [10, 25, 50, 75, 100]

best_transfer = {'r2': -np.inf}
best_regular = {'r2': -np.inf}
best_transfer_model = None
best_regular_model = None

for max_depth, n_est in itertools.product(max_depths, n_estimators_list):
    rs = np.random.RandomState(42)  # Reset seed for reproducibility

    # --- TwoStageTrAdaBoostR2 ---
    regr_transfer = TwoStageTrAdaBoostR2(
        base_estimator=DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5),
        n_estimators=n_est,
        sample_size=[len(X_source), len(X_target_train)],
        steps=30,
        fold=5,
        random_state=rs
    )
    regr_transfer.fit(np.concatenate((X_source, X_target_train), axis=0),
                      np.concatenate((y_source, y_target_train), axis=0))
    y_pred_transfer = regr_transfer.predict(X_target_test)
    r2_transfer = r2_score(y_target_test, y_pred_transfer)

    if r2_transfer > best_transfer['r2']:
        best_transfer = {
            'r2': r2_transfer,
            'rmse': np.sqrt(mean_squared_error(y_target_test, y_pred_transfer)),
            'mae': np.mean(np.abs(y_target_test - y_pred_transfer)),
            'mse': mean_squared_error(y_target_test, y_pred_transfer),
            'params': {'max_depth': max_depth, 'n_estimators': n_est}
        }
        best_transfer_model = regr_transfer

    # --- Regular AdaBoostR2 ---
    regr_regular = AdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5),
        n_estimators=n_est,
        random_state=42
    )
    regr_regular.fit(X_target_train, y_target_train)
    y_pred_regular = regr_regular.predict(X_target_test)
    r2_regular = r2_score(y_target_test, y_pred_regular)

    if r2_regular > best_regular['r2']:
        best_regular = {
            'r2': r2_regular,
            'rmse': np.sqrt(mean_squared_error(y_target_test, y_pred_regular)),
            'mae': np.mean(np.abs(y_target_test - y_pred_regular)),
            'mse': mean_squared_error(y_target_test, y_pred_regular),
            'params': {'max_depth': max_depth, 'n_estimators': n_est}
        }
        best_regular_model = regr_regular

# ================================================
#            Output Best Results
# ================================================

print("=== Best TwoStageTrAdaBoostR2 Results ===")
print("Params       :", best_transfer['params'])
print("R² Score     :", best_transfer['r2'])
print("RMSE         :", best_transfer['rmse'])
print("MSE          :", best_transfer['mse'])
print("MAE          :", best_transfer['mae'])

print("\n=== Best Regular AdaBoostR2 Results ===")
print("Params       :", best_regular['params'])
print("R² Score     :", best_regular['r2'])
print("RMSE         :", best_regular['rmse'])
print("MSE          :", best_regular['mse'])
print("MAE          :", best_regular['mae'])

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import itertools

# ================================================
#     Manual Grid Search for Both Models
# ================================================

max_depths = [2, 3]
n_estimators_list = [10, 25, 50]

best_transfer = {'r2': -np.inf}
best_regular = {'r2': -np.inf}
best_transfer_model = None
best_regular_model = None

for max_depth, n_est in itertools.product(max_depths, n_estimators_list):
    rs = np.random.RandomState(42)  # Reset seed for reproducibility

    # --- TwoStageTrAdaBoostR2 ---
    regr_transfer = TwoStageTrAdaBoostR2(
        base_estimator=DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5),
        n_estimators=n_est,
        sample_size=[len(X_source), len(X_target_train)],
        steps=30,
        fold=5,
        random_state=rs
    )
    regr_transfer.fit(np.concatenate((X_source, X_target_train), axis=0),
                      np.concatenate((y_source, y_target_train), axis=0))
    y_pred_transfer = regr_transfer.predict(X_target_test)
    r2_transfer = r2_score(y_target_test, y_pred_transfer)

    if r2_transfer > best_transfer['r2']:
        best_transfer = {
            'r2': r2_transfer,
            'rmse': np.sqrt(mean_squared_error(y_target_test, y_pred_transfer)),
            'mae': np.mean(np.abs(y_target_test - y_pred_transfer)),
            'mse': mean_squared_error(y_target_test, y_pred_transfer),
            'params': {'max_depth': max_depth, 'n_estimators': n_est}
        }
        best_transfer_model = regr_transfer

    # --- Regular AdaBoostR2 ---
    regr_regular = AdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=5),
        n_estimators=n_est,
        random_state=42
    )
    regr_regular.fit(X_target_train, y_target_train)
    y_pred_regular = regr_regular.predict(X_target_test)
    r2_regular = r2_score(y_target_test, y_pred_regular)

    if r2_regular > best_regular['r2']:
        best_regular = {
            'r2': r2_regular,
            'rmse': np.sqrt(mean_squared_error(y_target_test, y_pred_regular)),
            'mae': np.mean(np.abs(y_target_test - y_pred_regular)),
            'mse': mean_squared_error(y_target_test, y_pred_regular),
            'params': {'max_depth': max_depth, 'n_estimators': n_est}
        }
        best_regular_model = regr_regular

# ================================================
#            Output Best Results
# ================================================

print("=== Best TwoStageTrAdaBoostR2 Results ===")
print("Params       :", best_transfer['params'])
print("R² Score     :", best_transfer['r2'])
print("RMSE         :", best_transfer['rmse'])
print("MSE          :", best_transfer['mse'])
print("MAE          :", best_transfer['mae'])

print("\n=== Best Regular AdaBoostR2 Results ===")
print("Params       :", best_regular['params'])
print("R² Score     :", best_regular['r2'])
print("RMSE         :", best_regular['rmse'])
print("MSE          :", best_regular['mse'])
print("MAE          :", best_regular['mae'])

# prompt: train a adaboost.r2 with 'max_depth': 2, 'n_estimators': 10

# --- Train AdaBoostR2 with specified parameters ---
regr_regular_specific = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=2, min_samples_leaf=5),
    n_estimators=10,
    random_state=42
)
regr_regular_specific.fit(X_target_train, y_target_train)
y_pred_regular_specific = regr_regular_specific.predict(X_target_test)
r2_regular_specific = r2_score(y_target_test, y_pred_regular_specific)
rmse_regular_specific = np.sqrt(mean_squared_error(y_target_test, y_pred_regular_specific))
mse_regular_specific = mean_squared_error(y_target_test, y_pred_regular_specific)
mae_regular_specific = np.mean(np.abs(y_target_test - y_pred_regular_specific))

print("\n=== AdaBoostR2 Results with max_depth=2, n_estimators=10 ===")
print("R² Score     :", r2_regular_specific)
print("RMSE         :", rmse_regular_specific)
print("MSE          :", mse_regular_specific)
print("MAE          :", mae_regular_specific)

# === 1. Predict on training set ===
y_pred_regular_specific_train = regr_regular_specific.predict(X_target_train)

# === 2. Save Actual vs Predicted for Training Set ===
train_df_ada = pd.DataFrame({
    'Actual': y_target_train.flatten(),
    'Predicted': y_pred_regular_specific_train.flatten()
})
train_df_ada.to_csv("AdaBoost_TargetTrain_Actual_vs_Predicted.csv", index=False)

# === 3. Save Actual vs Predicted for Testing Set ===
test_df_ada = pd.DataFrame({
    'Actual': y_target_test.flatten(),
    'Predicted': y_pred_regular_specific.flatten()
})
test_df_ada.to_csv("AdaBoost_TargetTest_Actual_vs_Predicted.csv", index=False)

print("\n✅ AdaBoost.R2 actual vs predicted saved to:")
print("→ AdaBoost_TargetTrain_Actual_vs_Predicted.csv")
print("→ AdaBoost_TargetTest_Actual_vs_Predicted.csv")

import matplotlib.pyplot as plt
import matplotlib as mpl


# Create the plot
plt.figure(figsize=(10, 5), dpi=300)
plt.plot(y_target_test, label='Actual', linewidth=1.2)
plt.plot(y_pred_regular_specific, label='Predicted', linewidth=1.2)

# Label settings
plt.xlabel('Sample Index', fontsize=14)
plt.ylabel('USS', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
#plt.title('AdaBoost.R2 Prediction vs. Actual on Test Set', fontsize=14)
plt.legend(loc='upper right', fontsize=12)

# Tight layout and save
plt.tight_layout()
plt.savefig('adaboost.r2 graph.jpeg', dpi=300, format='jpeg')
plt.show()

"""# Two stage actual vs predicted"""

# prompt: now train a twostagetradaboost.r2 model for {'max_depth': 3, 'n_estimators': 10}
# and return evaluation matrices and similar visualization

# --- Train TwoStageTrAdaBoostR2 with specified parameters ---
rs = np.random.RandomState(42)
regr_transfer_specific = TwoStageTrAdaBoostR2(
    base_estimator=DecisionTreeRegressor(max_depth=3, min_samples_leaf=5),
    n_estimators=10,
    sample_size=[len(X_source), len(X_target_train)],
    steps=30,
    fold=5,
    random_state=rs
)
regr_transfer_specific.fit(np.concatenate((X_source, X_target_train), axis=0),
                           np.concatenate((y_source, y_target_train), axis=0))
y_pred_transfer_specific = regr_transfer_specific.predict(X_target_test)

r2_transfer_specific = r2_score(y_target_test, y_pred_transfer_specific)
rmse_transfer_specific = np.sqrt(mean_squared_error(y_target_test, y_pred_transfer_specific))
mse_transfer_specific = mean_squared_error(y_target_test, y_pred_transfer_specific)
mae_transfer_specific = np.mean(np.abs(y_target_test - y_pred_transfer_specific))

print("\n=== TwoStageTrAdaBoostR2 Results with max_depth=3, n_estimators=10 ===")
print("R² Score     :", r2_transfer_specific)
print("RMSE         :", rmse_transfer_specific)
print("MSE          :", mse_transfer_specific)
print("MAE          :", mae_transfer_specific)

# === 1. Predict on target training set ===
y_pred_transfer_specific_train = regr_transfer_specific.predict(X_target_train)

# === 2. Save Actual vs Predicted for Training Set ===
train_df_tstb = pd.DataFrame({
    'Actual': y_target_train.flatten(),
    'Predicted': y_pred_transfer_specific_train.flatten()
})
train_df_tstb.to_csv("TwoStageTrAdaBoost_TargetTrain_Actual_vs_Predicted.csv", index=False)

# === 3. Save Actual vs Predicted for Testing Set ===
test_df_tstb = pd.DataFrame({
    'Actual': y_target_test.flatten(),
    'Predicted': y_pred_transfer_specific.flatten()
})
test_df_tstb.to_csv("TwoStageTrAdaBoost_TargetTest_Actual_vs_Predicted.csv", index=False)

print("\n✅ TwoStageTrAdaBoost.R2 actual vs predicted saved to:")
print("→ TwoStageTrAdaBoost_TargetTrain_Actual_vs_Predicted.csv")
print("→ TwoStageTrAdaBoost_TargetTest_Actual_vs_Predicted.csv")

# Create the plot for TwoStageTrAdaBoostR2
plt.figure(figsize=(10, 5), dpi=300)
plt.plot(y_target_test, label='Actual', linewidth=1.2)
plt.plot(y_pred_transfer_specific, label='Predicted', linewidth=1.2)

# Label settings
plt.xlabel('Sample Index', fontsize=14)
plt.ylabel('USS', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.legend(loc='upper right', fontsize=12)

# Tight layout and save
plt.tight_layout()
plt.savefig('twostagetradaboost.r2 graph.jpeg', dpi=300, format='jpeg')
plt.show()

# prompt: return all the evaluation results for this trained twostagetradaboost.r2 model, both on target train and target test

# Predict on the target training set
y_pred_transfer_train = regr_transfer_specific.predict(X_target_train)

# Evaluate on the target training set
r2_transfer_train = r2_score(y_target_train, y_pred_transfer_train)
rmse_transfer_train = np.sqrt(mean_squared_error(y_target_train, y_pred_transfer_train))
mse_transfer_train = mean_squared_error(y_target_train, y_pred_transfer_train)
mae_transfer_train = np.mean(np.abs(y_target_train - y_pred_transfer_train))

print("\n=== TwoStageTrAdaBoostR2 Evaluation Results ===")
print("\n--- On Target Train Set ---")
print("R² Score     :", r2_transfer_train)
print("RMSE         :", rmse_transfer_train)
print("MSE          :", mse_transfer_train)
print("MAE          :", mae_transfer_train)

print("\n--- On Target Test Set ---")
print("R² Score     :", r2_transfer_specific)
print("RMSE         :", rmse_transfer_specific)
print("MSE          :", mse_transfer_specific)
print("MAE          :", mae_transfer_specific)

# Create the plot for TwoStageTrAdaBoostR2
plt.figure(figsize=(10, 5), dpi=300)
plt.plot(y_target_test, label='Actual', linewidth=1.2)
plt.plot(y_pred_transfer_specific, label='Predicted', linewidth=1.2)

# Label settings
plt.xlabel('Sample Index', fontsize=14)
plt.ylabel('USS', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.legend(loc='upper right', fontsize=12)

# Tight layout and save
plt.tight_layout()
plt.savefig('twostagetradaboost.r2 graph.jpeg', dpi=300, format='jpeg')
plt.show()

print("\n=== Best TwoStageTrAdaBoostR2 Parameters ===")
print(best_transfer['params'])

print("\n=== Best Regular AdaBoostR2 Parameters ===")
print(best_regular['params'])

"""# Ablation study"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Font Configuration (Times New Roman)
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']

# Split target into ablation pool and fixed test set
X_target = target_df[['LL', 'PL', 'PI', 'Moisture', 'Density']].values
y_target = target_df['USS'].values
Xa_full, Xt, ya_full, yt = train_test_split(X_target, y_target, test_size=0.2, random_state=42)

# Scale source and target data
scaler = StandardScaler()
X_source_scaled = scaler.fit_transform(X_source)
Xa_full_scaled = scaler.transform(Xa_full)
Xt_scaled = scaler.transform(Xt)

# TwoStageTrAdaBoost.R2 parameters
tradaboost_params = {
    'base_estimator': DecisionTreeRegressor(max_depth=3, min_samples_leaf=5),
    'n_estimators': 10,
    'steps': 30,
    'fold': 2,
    'random_state': np.random.RandomState(42)
}

# AdaBoost.R2 baseline parameters
adaboost_params = {
    'estimator': DecisionTreeRegressor(max_depth=3),
    'n_estimators': 10,
    'random_state': 42
}

# Ablation study setup
target_percents = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
n_runs = 40  # stability

# Arrays to store metrics
r2_scores_tradaboost_all = np.zeros((n_runs, len(target_percents)))
r2_scores_adaboost_all = np.zeros((n_runs, len(target_percents)))
rmse_scores_tradaboost_all = np.zeros((n_runs, len(target_percents)))
rmse_scores_adaboost_all = np.zeros((n_runs, len(target_percents)))

# Main loop
for i in range(n_runs):
    shuffled_indices = np.random.permutation(len(Xa_full_scaled))
    Xa_full_shuffled = Xa_full_scaled[shuffled_indices]
    ya_full_shuffled = ya_full[shuffled_indices]

    for j, perc in enumerate(target_percents):
        n_samples = int(len(Xa_full_shuffled) * perc)
        if n_samples < 2:
            continue

        Xa_sub = Xa_full_shuffled[:n_samples]
        ya_sub = ya_full_shuffled[:n_samples]

        # TwoStageTrAdaBoost.R2
        model_tradaboost = TwoStageTrAdaBoostR2(
            base_estimator=tradaboost_params['base_estimator'],
            n_estimators=tradaboost_params['n_estimators'],
            sample_size=[len(X_source_scaled), len(Xa_sub)],
            steps=tradaboost_params['steps'],
            fold=tradaboost_params['fold'],
            random_state=tradaboost_params['random_state']
        )
        model_tradaboost.fit(
            np.concatenate((X_source_scaled, Xa_sub), axis=0),
            np.concatenate((y_source, ya_sub), axis=0)
        )
        y_pred_tradaboost = model_tradaboost.predict(Xt_scaled)
        r2_tradaboost = r2_score(yt, y_pred_tradaboost)
        rmse_tradaboost = np.sqrt(mean_squared_error(yt, y_pred_tradaboost))
        r2_scores_tradaboost_all[i, j] = r2_tradaboost
        rmse_scores_tradaboost_all[i, j] = rmse_tradaboost

        # AdaBoost.R2 (Baseline)
        model_adaboost = AdaBoostRegressor(**adaboost_params)
        model_adaboost.fit(Xa_sub, ya_sub)
        y_pred_adaboost = model_adaboost.predict(Xt_scaled)
        r2_adaboost = r2_score(yt, y_pred_adaboost)
        rmse_adaboost = np.sqrt(mean_squared_error(yt, y_pred_adaboost))
        r2_scores_adaboost_all[i, j] = r2_adaboost
        rmse_scores_adaboost_all[i, j] = rmse_adaboost

# --- Aggregate metrics ---
# R²
mean_r2_tradaboost = np.mean(r2_scores_tradaboost_all, axis=0)
ci_r2_tradaboost_low = np.percentile(r2_scores_tradaboost_all, 2.5, axis=0)
ci_r2_tradaboost_high = np.percentile(r2_scores_tradaboost_all, 97.5, axis=0)

mean_r2_adaboost = np.mean(r2_scores_adaboost_all, axis=0)
ci_r2_adaboost_low = np.percentile(r2_scores_adaboost_all, 2.5, axis=0)
ci_r2_adaboost_high = np.percentile(r2_scores_adaboost_all, 97.5, axis=0)

# RMSE
mean_rmse_tradaboost = np.mean(rmse_scores_tradaboost_all, axis=0)
ci_rmse_tradaboost_low = np.percentile(rmse_scores_tradaboost_all, 2.5, axis=0)
ci_rmse_tradaboost_high = np.percentile(rmse_scores_tradaboost_all, 97.5, axis=0)

mean_rmse_adaboost = np.mean(rmse_scores_adaboost_all, axis=0)
ci_rmse_adaboost_low = np.percentile(rmse_scores_adaboost_all, 2.5, axis=0)
ci_rmse_adaboost_high = np.percentile(rmse_scores_adaboost_all, 97.5, axis=0)

# --- Visualization ---
fig, axes = plt.subplots(1, 2, figsize=(10, 4))
target_percents_plot = np.array(target_percents) * 100

# Subplot 1: R² Score
axes[0].plot(target_percents_plot, mean_r2_tradaboost, 'o-', label="TwoStageTrAdaBoost.R2", color='tab:red', linewidth=1)
axes[0].fill_between(target_percents_plot, ci_r2_tradaboost_low, ci_r2_tradaboost_high, color='tab:red', alpha=0.2)
axes[0].plot(target_percents_plot, mean_r2_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
axes[0].fill_between(target_percents_plot, ci_r2_adaboost_low, ci_r2_adaboost_high, color='tab:blue', alpha=0.18)

axes[0].set_xlabel("Target domain data (%)", fontsize=14)
axes[0].set_ylabel("R² Score", fontsize=14)
axes[0].set_title("(a) Coefficient of Determination ($R^2$)", fontsize=14)
axes[0].grid(True, linestyle='--', alpha=0.7)
axes[0].set_xticks(target_percents_plot)
axes[0].set_ylim(0, 1.0)
axes[0].legend()

# Subplot 2: RMSE
axes[1].plot(target_percents_plot, mean_rmse_tradaboost, 'o-', label="TwoStageTrAdaBoost.R2", color='tab:red', linewidth=1)
axes[1].fill_between(target_percents_plot, ci_rmse_tradaboost_low, ci_rmse_tradaboost_high, color='tab:red', alpha=0.2)
axes[1].plot(target_percents_plot, mean_rmse_adaboost, 's--', label="Baseline AdaBoost.R2", color='tab:blue', linewidth=1)
axes[1].fill_between(target_percents_plot, ci_rmse_adaboost_low, ci_rmse_adaboost_high, color='tab:blue', alpha=0.18)

axes[1].set_xlabel("Target domain data (%)", fontsize=14)
axes[1].set_ylabel("RMSE", fontsize=14)
axes[1].set_title("(b) Root Mean Square Error (RMSE)", fontsize=14)
axes[1].grid(True, linestyle='--', alpha=0.7)
axes[1].set_xticks(target_percents_plot)
axes[1].set_ylim(bottom=0)
axes[1].legend()

plt.suptitle("TwoStageTrAdaBoost.R2 Performance vs. Baseline Model", fontsize=14, y=1.02)
plt.tight_layout()
plt.show()

fig.savefig('twostage vs ML performance v2.svg', format='svg', dpi=1200)